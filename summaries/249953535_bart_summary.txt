Existing evaluations are too object-focused, artificially creating "novelty" by repurposing images designed for object classification. Our dataset can be used to prototype multimodal methods, which are thus far under-explored in the literature. Autoencoder performs best followed by the One-Class SVM and NDCC. GCD SSKM. We use a ViT-B-16 vision transformer pretrained with DINO (Caron et al., 2021) self-supervision on unlabeled ImageNet. Fine-tuning is done with the GCD contrastive loss with supervised loss weight 0.35. We train the predictive models using the Adam optimizer with a learning rate of 10 âˆ’4 for 50 epochs.