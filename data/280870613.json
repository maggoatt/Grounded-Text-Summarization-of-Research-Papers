{
  "corpusid": 280870613,
  "title": "Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective",
  "authors": [
    "M. Bhuiyan",
    "Matteo Varvello",
    "Yasir Zaki",
    "Cristian-Alexandru Staicu"
  ],
  "url": "https://arxiv.org/abs/2508.18328",
  "license": null,
  "sections": [
    {
      "section_title": "INTRODUCTION",
      "text": "An estimated 2.2 billion people live with some form of visual impairment, and 90% of them reside in low-and middleincome countries [44]. As internet adoption grows in these regions, an increasing share of web content is created in languages other than English. Many of these languages use non-Latin writing systems such as Devanagari, Bengali, Arabic, or Thai and are often presented alongside English in mixed-language interfaces [28][29][30]. Popular screen readers like JAWS [2] and NVDA [7] still exhibit limited support for non-Latin scripts and often perform poorly when confronted with mixed languages [14,25,40,41]. These tools may misrender non-English words or produce unintelligible output, as has been documented with languages like Nepali [38].\n\nThe root of the problem lies in the inadequate usage of text alternatives [12,24] metadata such as lang attributes or image alt text, which screen readers use to process content appropriately. When such metadata is absent, incorrect, or inconsistent with the visible text, it creates a mismatch between the displayed content and what the assistive tool offers. This issue is compounded for scripts that require complex shaping or language-specific pronunciation models. For example, Apple's VoiceOver [11], the default screen reader on macOS and iOS devices, does not provide any support for languages such as Urdu, Amharic, or Burmese [11]. This lack of linguistic inclusivity is at odds with the principles laid out by the W3C Web Accessibility Initiative, which advocates for equal digital access regardless of language or ability [43].\n\nWeb accessibility research only marginally studies the intersection of multilingualism and assistive technology [17,18]. A key bottleneck in this area is the lack of data. Widely used datasets like Tranco [31] focus on popularity but do not provide any insight into the linguistic composition of websites. This limits the researchers' ability to measure the prevalence of multilingual content and to assess the accessibility landscape for speakers of less commonly supported languages. To bridge this gap, we introduce LangCrUX, a largescale dataset of 120,000 popular websites across 12 languages that primarily use non-Latin scripts. We construct LangCrUX by selecting high-traffic websites from the Chrome User Experience Report (CrUX) dataset, verifying language use via automated script detection and manual sampling, and through Puppeteer-based crawls routed via country-specific VPNs to capture localized versions of each site.\n\nWe leverage LangCrUX to conduct the first large-scale analysis of multilingual web accessibility, focusing on how assistive technologies interact with real-world, multilingual web content. Our findings reveal that nearly 40% of websites in Bangladesh and India lack any accessibility text in the native language, despite having predominantly native-language visible content. More broadly, language-aware accessibility remains a widespread and under-addressed issue: many websites use non-descriptive, placeholder, or untranslated text (e.g., \"file1, \" \"button, \" or generic English terms) in critical accessibility elements like image alt text. These mismatches significantly reduce the utility of screen readers, which rely on metadata to convey meaningful information to users. We observe that current automated accessibility testing tools fail to detect these language inconsistencies, as they typically only evaluate the presence of accessibility hints. To address this gap, we propose and develop Kizuki 1 , a testing extension that identifies such mismatches and evaluates metadata based on alignment with the surrounding linguistic context, offering a more inclusive measure of accessibility."
    },
    {
      "section_title": "METHODOLOGY",
      "text": "Our study investigates the accessibility of websites in non-Latin-script languages, focusing on linguistic diversity in underrepresented language communities. We collect LangCrUX, a dataset comprising websites with a high proportion of non-Latin-script content, combining data from the Chrome User Experience Report (CrUX) [8], custom web crawls using Puppeteer, and language metadata verification. Figure 1 visualizes our methodology, including dataset construction, language selection, and automated accessibility analysis.\n\nLangCrUX is released as an open-source dataset on GitHub. 2\n\nLanguage and Country Selection Criteria: We begin with a pool of 26 widely spoken non-Latin-script languages, including Hindi, Bangla, Modern Standard Arabic, Tamil, Telugu, Mandarin Chinese, Urdu, Amharic, Russian, Marathi, and others [6]. Our goal is to identify a diverse, representative set of languages underrepresented in web accessibility research, particularly those using non-Latin writing systems. Language selection is guided by three main factors: script type (non-Latin), size of the global speaker base, and geographic and linguistic diversity. To ensure representativeness 1 named after the Japanese word for \"awareness\" 2 https://anonymous.4open.science/r/LangCrux-F68F Figure 2: Native language distribution in visible text for India and Israel in LangCrUX and sufficient data, we apply two strict inclusion criteria: 1) at least 10,000 websites with 50% or more visible textual content in the target language, and 2) inclusion in the CrUX datasetwhich provides user experience metrics from Chrome userswith sufficient traffic and performance data. Applying these filters results in a final set of 12 language-country pairs, each uniquely representing a distinct non-Latin-script language with a verifiable web presence. Because the initial pool alone did not yield enough languages that met these thresholds, we expanded the selection to include additional ones such as Hebrew, Sinhala, Greek, and Burmese. These were added to increase script and regional diversity while still satisfying our inclusion criteria.\n\nThe selected countries, their corresponding languages, and approximate global speaker populations are China (Mandarin Chinese, 1.2 billion), India (Hindi, 609 million), Algeria (Modern Standard Arabic, 335 million), Bangladesh (Bangla, 284 million), Russia (Russian, 253 million), Japan (Japanese, 126 million), Egypt (Egyptian Arabic, 119 million), Hong Kong (Cantonese, 85.5 million), South Korea (Korean, 82 million), Thailand (Thai, 71 million), Greece (Greek, 13.5 million), and Israel (Hebrew, 9 million). Collectively, these 12 languages are spoken by over 3.19 billion people, representing about 39.5% of the global population. For languages spoken in multiple countries, such as Modern Standard Arabic, used in Algeria, Saudi Arabia, and Morocco, we select the country with the highest population of native speakers, in this case, Algeria. In multilingual countries like India, we include all major non-Latin-script languages with substantial speaker populations. However, only Hindi meets our data threshold; other widely spoken languages, such as Tamil and Telugu, do not meet the 10,000-website requirement and are excluded. Similar exclusions apply to Sinhala (Sri Lanka) and Georgian (Georgia), where websites with sufficient native-language content fell below the threshold despite initial inclusion.\n\nWebsite Selection: We use Google CrUX to identify and rank websites by real-world usage metrics. For each selected language-country pair, we extract the top 10,000 websites based on CrUX rankings, which reflect user engagement, load performance, and interaction quality. To validate language presence, we use a Unicode-based heuristic that matches visible text content against script-specific character ranges (e.g., Devanagari for Hindi, Hangul for Korean, and Cyrillic for Russian). For overlapping scripts, such as Arabic and Urdu, we include additional language-specific characters to improve precision. A website is retained if at least 50% of its visible textual content is in the target language. Websites that do not meet this threshold are excluded and replaced with the next-ranking candidate from the CrUX list. In cases where 10,000 qualifying websites could not be found among the top-ranked entries, we extended our search to lowerranked websites within the CrUX database to fulfill the quota. Figure 2 illustrates the distribution of visible content by language for two representative cases: Hindi websites in India and Hebrew websites in Israel.\n\nData Collection: To extract accessibility-related features from the selected websites, we develop a web crawler using Puppeteer [9], which simulates web browsing conditions in a Chromium environment. Each website is visited programmatically, allowing us to capture network-level metadata, page structure, and accessibility indicators such as alternative text, ARIA (Accessible Rich Internet Applications) attributes (which enhance the semantics of web elements for assistive technologies [1]), and declared language tags.\n\nTo capture the localized experience of users in each country, we route all browser traffic through VPN servers physically hosted in the corresponding country. This step is critical for collecting region-specific versions of websites, as many sites dynamically serve content-including language settings, layout, or accessibility features-based on the user's IP location. Without VPN-based localization, web crawlers risk accessing global or English-dominant versions of websites that do not accurately reflect the intended user experience of native speakers. We use a combination of commercial VPN services, including ProtonVPN [10] and Hotspot Shield [23], to achieve broad geographic coverage. Since not all VPN providers have servers in every target country, we select the provider on a per-country basis to ensure reliable and consistent access from within national borders. Compared to crawling from generic cloud-hosted IPs, this approach offers a significantly more realistic vantage point, reducing the risk of content variation, redirection, or censorship artifacts that may otherwise skew accessibility analysis."
    },
    {
      "section_title": "Accessibility Element Selection Criteria:",
      "text": "To identify accessibility elements where natural language plays a critical role, we follow a structured process based on the Lighthouse accessibility auditing framework [20]. Lighthouse evaluates a range of accessibility checks, each associated with specific HTML elements and best practices. Our goal is to select those elements for which the presence, clarity, and appropriateness of natural language directly influence accessibility outcomes."
    },
    {
      "section_title": "button-name document-title image-alt frame-title summary-name label input-image-alt select-name link-name input-button-name svg-img-alt object-alt",
      "text": "Table 1: Web elements requiring natural language.\n\nWe begin by examining the set of Lighthouse accessibility tests, which internally relies on the Axe-core accessibility engine [5]. For each test, we identify the corresponding HTML element it targets (e.g., <img>, <button>, <input>). We then analyze the test rationale by referencing the associated rule definitions from Axe-core, which provide detailed explanations and criteria for each audit. From these specifications, we assess whether natural language content is integral to the test. Specifically, we ask whether the accessibility of the element depends on human-readable text, for example, whether a screen reader user would rely on the clarity and relevance of that text to understand the element's purpose.\n\nIf natural language is central to the function or evaluation of the element, we include it in our set. Following this process, we identify the twelve elements listed in Table 1 as language-sensitive accessibility features.\n\nThese elements span a range of interface components, including images, forms, buttons, and navigation elements, all of which rely on meaningful textual descriptions to be accessible to users with visual impairments. We explicitly exclude certain tests, such as video-caption. Although captions are inherently language-dependent and critical for accessibility, accurately evaluating them at scale poses challenges. In many cases, captions are not embedded in the HTML but are provided through separate files (e.g., VTT or SRT) or dynamically loaded via JavaScript. These may be inaccessible to crawlers unless the video is played or fully rendered in the browser. Furthermore, identifying whether a video has accurate, synchronized, and complete captions often requires manual inspection or audio-visual comparison-steps outside the scope of automated large-scale analysis. Due to these limitations, we omit video-caption checks to maintain consistency and reproducibility in our methodology.\n\nLimitations: Our methodology has several limitations. First, reliance on CrUX limits our scope to websites with measurable Chrome traffic, potentially excluding low-traffic or highly localized websites. Next, while using a VPN allows accessing region-specific versions of websites, some websites may detect VPN use and return generic or restricted versions. In such cases, we replace the affected websites with the next eligible candidate. Additionally, Puppeteer's simulated browsing environment may not fully reflect user experiences. Finally, language detection (both automated and manual) can be challenged by embedded content or non-standard scripts, though our 50% content threshold and manual verification aim to reduce this risk."
    },
    {
      "section_title": "IS MULTILINGUAL WEB ACCESSIBLE?",
      "text": "Table 2 provides statistics on the quality and presence of accessibility text across twelve HTML elements. For each element, we report the median, standard deviation, and average percentage of websites where the accessibility attribute is missing or empty. We also include metrics like text length (in characters) and word count to assess richness and verbosity. These measurements allow us to compare how frequently accessibility features are implemented and how informative they are when present. In the following, we analyze these patterns by examining missing and empty values, evaluating text length and word count, filtering uninformative content, and characterizing the language distribution of informative accessibility text."
    },
    {
      "section_title": "Prevalence of Missing and Empty Accessibility Texts:",
      "text": "Missing accessibility text is a widespread issue across HTML elements. Several elements exhibit extremely high average missing rates, including label (98.5%), svg-img-alt (96.6%), link-name (95.9%), input-button-name (93.9%), and summary-name (90.47%). The image-alt attribute stands out for its relatively lower average missing rate (17.12%) but exhibits the highest percentage of empty values (25.39%). Although our findings show that it is possible to pass the Lighthouse audit for image accessibility by setting the alt attribute to an empty string, such text does not convey meaningful information to users (Appendix D). Compared to non-multilingual websites, the missing percentage is slightly higher (15.19% vs. 17.12%) and the empty percentage is notably higher (16.36% vs. 25.39%), indicating a greater tendency to include but not meaningfully populate the attribute in multilingual contexts.\n\nAttributes like button-name and link-name, which are essential for identifying interactive components, also show high missing rates (61.92% and 95.96%, respectively). Similarly, attributes associated with form controls, such as input -button-name and select-name, are frequently absent or left empty, compromising the clarity of form interactions. A likely reason for these high rates is that, in many cases, screen readers fall back to reading visible HTML text (such as the inner text of a button or link) when accessibility attributes like aria-label, label, or alt are missing. This fallback behavior reduces the perceived need for developers to explicitly include accessibility metadata, especially when the element already includes visible text.\n\nText Length and Word Count Analysis: Table 2 also captures the descriptive quality of accessibility text through text length and word count metrics. Among the elements, link-name has a relatively high average text length (∼27 characters) and word count (∼5), compared to summary-name, select-name, label, and button-name, which show lower average word counts (1.17, 2.31, 1.67, and 3.86, respectively). This suggests that link descriptions tend to be more detailed and contextually informative. However, for some other elements, shorter texts are often acceptable; for example, buttons labeled \"Login,\" \"Send,\" or \"Submit\" typically provide sufficient clarity with just one or two words.\n\nFor elements like image-alt, which require contextual descriptions to convey the meaning of an image, we find an average word count of only ∼4. This shortness, especially when combined with the high empty rate noted earlier, suggests a broader trend of developers including minimal or superficial alt text, possibly to satisfy automated checks rather than to genuinely enhance accessibility. Finally, the table reveals substantial outliers, e.g., image-alt has a maximum of 261,864 characters and 12,306 words, while link-name 5,228 characters and 518 words. These extreme but rare cases likely indicate instances where extraneous content, such as metadata, boilerplate text, or full paragraphs, has been mistakenly inserted into accessibility attributes, potentially overwhelming assistive technologies and undermining user experience (see Appendix E for examples)."
    },
    {
      "section_title": "Filtering Uninformative Accessibility Text:",
      "text": "To assess the quality of accessibility text, we apply a filtering step to discard uninformative or placeholder texts. This is essential because the presence of an alt or aria-label attribute does not guarantee usefulness. Labels such as button, file1, or image1 may satisfy automated checks but provide no semantic value to screen reader users. We define a set of heuristics to classify accessibility texts into eleven categories, distinguishing between useful and discardable content. These include short strings, file paths, placeholders, developer labels, etc. Appendix H shows the full list of filtering rules. Appendix G shows the breakdown by HTML element.\n\nFigure 3 shows the percentage distribution of filtered accessibility texts across countries and categories. One of the most common issues is the use of generic single words. For example, in Thailand, over 33% of accessibility texts are singleword labels. High rates are also observed in Russia (22.2%), Greece (18.03%), and India (17.1%). In contrast, countries like Bangladesh (6.9%) and Egypt (10.5%) show lower proportions. A small but non-negligible portion of texts are too short to convey meaning. In Russia, 4.26% of labels fall into this category, followed by Thailand (4.24%), Israel (4.03%), and India (3.6%). Some websites also use raw URLs or file  paths as labels. This affects 3.8% of labels in Hong Kong, 3.5% in South Korea, and 3.17% in Russia. These findings show that even when accessibility text is present, it often lacks descriptive content. This reinforces the need to go beyond presence-based metrics and assess the actual semantic value of accessibility labels."
    },
    {
      "section_title": "Language Distribution of Informative Accessibility Text:",
      "text": "After removing uninformative and placeholder accessibility text, we reanalyze the remaining content to understand the language distribution of texts that are potentially useful. This filtered set reflects more intentional and meaningful uses of alt, label, and other accessibility attributes. Figure 4 shows the proportion of accessibility texts written in native languages, English, or a mix of both, across the 12 analyzed countries. A prominent pattern is the heavy reliance on English, even in countries where it is not the primary language. In Bangladesh, 79% of informative accessibility texts are in English, the highest among all countries analyzed. Egypt, Thailand, and Greece also show a strong tendency to default to English. This suggests that developers may rely on English for accessibility metadata due to limited localization tools or being unaware of screen reader needs in native languages. Another important pattern is the use of mixed-language accessibility hints, where a single alt attribute contains both the native language and English. This occurs frequently in Greece (35%), Thailand (34%), and Hong Kong (30%), and in over 20% of websites in China, Russia, Japan, and India. While sometimes intended to aid multilingual users, such mixing often confuses screen readers, which typically do not handle language switching within a single label, resulting in mispronunciations or reduced clarity."
    },
    {
      "section_title": "LANGUAGE-AWARE ACCESSIBILITY",
      "text": "Mismatch Between Visible and Accessibility Text: Figure 5 compares native language usage in visible versus accessibility text across the 12 analyzed countries. While the visible content of many websites is multilingual or predominantly in the native language, the associated accessibility metadata, such as alt text, aria-labels, and form labels, is typically written in English. This mismatch is especially noticeable in countries like India and Bangladesh, where over 40% of websites have less than 10% of their accessibility text in the native language. Thailand, China, and Hong Kong also show similar trends, with more than a quarter of their websites falling into this category. In contrast, countries like Japan and Israel have significantly lower rates of mismatch, with fewer than 10% of websites showing such disparities. For blind users who rely on screen readers, this language discrepancy introduces an additional barrier, forcing them to navigate a bilingual interface where visible content and assistive text do not align. Appendix F provides a detailed view of the mismatch by visualizing the distribution of visible versus accessibility text across all countries in our dataset.\n\nFor example, in Bangladesh, https://teachers.gov.bd is a widely used government education portal. Although more than 98% of its visible content is in Bangla, only one of the 79 images with alt attributes uses Bangla. Comparable patterns are also seen in India, Thailand, and China. The Indian website https://cmhelpline.mp.gov.in has a Hindi version where the interface is almost entirely in Hindi, but all accessibility text is in English. The Thai news website https://www. khaosod.co.th contains over 92% of its visible content in Thai, but its accessibility labels are mostly in English. The Chinese provincial government site https://kjt.shaanxi.gov.cn is almost fully in Chinese, yet its accessibility texts are entirely in English. Mismatch examples are provided in Appendix I.\n\nAdding Language Awareness to Lighthouse: Automated testing tools such as Lighthouse do not consider the language of accessibility text when evaluating compliance. As a result, alt attributes are marked as present regardless of whether their content matches the language of the surrounding interface. To address this limitation, we introduce Kizuki, a Lighthouse extension that incorporates language awareness in accessibility evaluation. Specifically, we extend the audit for image alt text to verify whether the description is written in the same language as the page's visible content.\n\nWe evaluate Kizuki on 10,000 websites from Bangladesh and Thailand, two countries where language mismatch between visible content and accessibility metadata is particularly common. For fairness, we exclude websites that fail the original Lighthouse test due to missing alt attributes. Figure 6 shows the resulting shift in accessibility scores. Without considering language, 43% of websites received a Lighthouse score above 90 (considered \"good\" [3,4]), and 5.6% achieved a perfect score. After applying Kizuki's language-aware check, these numbers dropped significantly: only 15.8% of websites scored above 90, and just 1.8% retained a perfect score."
    },
    {
      "section_title": "RELATED WORK",
      "text": "Several studies have explored the challenges of multilingual web accessibility. Vázquez et al. [33,34,36,42] conducted user studies highlighting the limitations of screen readers in handling multilingual interfaces. Casalegno [15]  reported similar findings, emphasizing the cognitive strain users face when navigating mixed-language content. García-Garcinuño et al. [17] confirmed that language mismatches in screen reader output persist even when markup is correctly annotated. Vázquez et al. [35,37] also showed that accessibility is often excluded from localization workflows, leading to untranslated alt text and inconsistent metadata. Several works highlight problems such as mispronunciation, broken accents, or robotic voices in the context of multilingual content for screen readers [25,40,41]. Sankhi et al. [38] documented similar barriers in Nepal, while Raghavendra et al. [32] pointed to the lack of robust multilingual speech synthesis systems in Indian languages, emphasizing infrastructural challenges for regional screen reader development.\n\nResearchers also explored automated solutions to tackle accessibility issues. Several approaches are proposed for alt text generation, including human-curated [19,46], image-searchbased [21,27], and AI-based methods [13,16,45]. While human and search-based approaches emphasize contextual accuracy and reusability, AI-driven techniques offer scalable alternatives, including generating captions [22,26,39] or even producing the image itself with embedded descriptions [13]. However, these methods still rely on high-quality training data and often require human oversight to ensure contextual relevance and inclusivity."
    },
    {
      "section_title": "CONCLUSION",
      "text": "Multilingual web content is increasing in prevalence, but accessibility support lags behind, especially for non-Latin scripts. This paper introduces LangCrUX, the first large-scale dataset of 120,000 popular websites across 12 languages that primarily use non-Latin scripts. Analysis of the LangCrUX dataset reveals widespread issues and motivates languageaware accessibility improvements such as Kizuki, a Google Lighthouse extension we developed, which incorporates language consistency checks into accessibility testing. We hope that our dataset, which we will open source, will spark the community's interest to further measure the implications of an increasingly multilingual web."
    },
    {
      "section_title": "A ETHICS",
      "text": "This work does not raise any ethical issues."
    },
    {
      "section_title": "B DATA AND TOOL AVAILABILITY",
      "text": "We have open-sourced both the LangCrUX dataset and Kizuki. Kizuki includes detailed documentation and a README file explaining how to use it and how to extend it with custom accessibility tests. We have also created an interactive website for LangCrUX, where users can explore the dataset in greater detail, including language distribution across individual websites, with sampling and filtering options.\n\nThe dataset and testing tool are available at: https://anonymous.4open.science/r/LangCrux-F68F/ The interactive website is available at: https://anonymous.4open.science/w/LangCrux-F68F/"
    },
    {
      "section_title": "E EXAMPLES OF EXTREME ACCESSIBILITY TEXT VALUES",
      "text": "Table 4 shows examples of image alt texts that exceed 1000 characters. These values were extracted from real-world webpages from Bangladesh, India, Japan, Greece, and Thailand, and are shown along with the corresponding source URLs. The examples illustrate cases where accessibility attributes contain unusually long descriptive content, often including entire paragraphs or embedded metadata."
    },
    {
      "section_title": "F COUNTRY-LEVEL SCATTER PLOTS",
      "text": "To complement the main analysis, we include country-specific scatter plots, shown in Figure 8, displaying the distribution of websites based on the percentage of native-language content in visible versus accessibility text. Each point represents one site, with the x-axis representing the share of visible content in the native language, and the y-axis representing the share of accessibility metadata in the same language. These plots offer a more detailed view of the language mismatch patterns discussed in Section §3. As an illustration, consider the scatter plot for Thai websites, shown in Figure 8k. Points near the bottom of the plot represent websites where there is almost no accessibility metadata in Thai, regardless of the language used in the visible content. A dense cluster in the bottom right corner indicates websites where the visible content is almost entirely in Thai, but the accessibility text includes little or no Thai. In contrast, points in the top right corner represent websites where both the visible and accessibility content are predominantly in Thai, indicating consistent language use across both types of content."
    },
    {
      "section_title": "G ELEMENT-LEVEL FILTERING ANALYSIS",
      "text": "Figure 9 shows the distribution of uninformative accessibility text by HTML element. Generic action labels are especially common in <button> (14.2%) and <input> buttons (13.5%), indicating vague, non-descriptive usage. Single-word labels are the most prevalent issue overall, notably in <label> (24.4%), <image-alt> (17.1%), and <select> (15.3%) elements. These short, generic strings often fail to provide meaningful context. <summary> elements show both high generic action (42.9%) and single-word rates (40.5%), highlighting minimal semantic value. These trends suggest a need for deeper evaluation of accessibility text beyond its presence."
    },
    {
      "section_title": "H FILTERING UNINFORMATIVE ACCESSIBILITY TEXT",
      "text": "To assess the informativeness of accessibility text (e.g., alt, aria-label, or label attributes), we apply a rule-based filtering pipeline to discard generic or low-quality entries. Below, we outline each discard category, its rationale, and representative examples."
    },
    {
      "section_title": "I EXAMPLES OF MISMATCH BETWEEN VISIBLE AND ACCESSIBILITY TEXT",
      "text": "Table 5 illustrates examples of accessibility mismatches on six websites across Bangladesh, India, Thailand, Egypt, China, and Hong Kong. Each cell consists of an image from the website, the corresponding URL, and the alt text or accessibility description associated with that image. The examples highlight cases where website content is presented in the native language, yet the accessibility descriptions, such as alt texts, are written in English. This language inconsistency creates confusion for screen reader users and demonstrates the need for language-aligned accessibility practices."
    }
  ]
}