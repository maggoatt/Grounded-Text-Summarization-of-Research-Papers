{
  "corpusid": 249953535,
  "title": "NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds",
  "authors": [
    "Patrick Feeney",
    "Sarah Schneider",
    "Panagiotis Lymperopoulos",
    "Liping Liu",
    "matthias. scheutz",
    "M. E. C. Science",
    "T. University",
    "Center for Vision",
    "Automation",
    "Control",
    "Austrian Institute of Technology"
  ],
  "url": "https://arxiv.org/abs/2206.11736",
  "license": null,
  "sections": [
    {
      "section_title": "Introduction",
      "text": "Recent progress in computer vision (Krizhevsky et al., 2017) and vision-informed reinforcement learning (Mnih et al., 2015;Silver et al., 2018) is exciting but focused on tasks like classification or video game playing where the agent's goals are narrowly defined in a world that does not change. Many real applications require agents that can navigate open worlds that evolve over time (Boult et al., 2019). Designing effective agents for these domains requires the ability to detect, characterize, and ultimately adapt to novel stimuli.\n\nPublished in Transactions on Machine Learning Research (03/2023) To define novelty, we take the point of view of an agent with some prior experience in a given environment. We define a stimuli as novel to the agent if it considerably differs from the agent's model (Ruff et al., 2021). Some authors refer to similar problems as anomaly detection or out-of-distribution detection. Novelties are not simply outliers. Detecting a novelty suggests that revising the agent's model is necessary, whereas an outlier is a rare event expected (however infrequently) under the existing model.\n\nWork on novelty detection has spanned decades (Markou & Singh, 2003;Pimentel et al., 2014;Ruff et al., 2021), with a wealth of available paradigms spanning \"one-class\" boundary learning (Schölkopf et al., 2000), reconstruction error (Abati et al., 2019), probabilistic density modeling (Nalisnick et al., 2018), and more. For vision-based novelty detection, recent progress has been dominated by modifications of deep image classifiers (Lakshminarayanan et al., 2017;Liang et al., 2018;Cheng & Vasconcelos, 2021).\n\nDespite this wealth of methods, evaluation practices have seen less attention. Recent visual novelty detection systems are typically evaluated by repurposing object-focused datasets such as Caltech-256 (Griffin et al., 2007) or CIFAR-10 (Krizhevsky, 2009) that were originally intended for supervised classification. A typical evaluation paradigm trains on a \"normal\" subset of available classes, then tries to detect \"novel\" images from other classes unseen during training. We contend that when using object-focused datasets, this paradigm is too optimistic, representing best-case scenarios where the object is relatively large and well-framed within the image. While some efforts have turned toward fine-grained classification datasets like distinguishing novel breeds of dogs (Cheng & Vasconcelos, 2021), these images remain object-focused. To develop agents ready to navigate open worlds, we argue that a scene-focused benchmark is needed.\n\nWe have developed and released a comprehensive scene-focused dataset that we call NovelCraft, for benchmarking systems that can detect and adapt to novelty in open worlds. Our target domain is a modified Left: Images from two normal episodes (rows 1-2) and two novel episodes (rows 3-4). Detection is challenging as only a few images in novel episodes actually contain novel objects, here outlined in orange when visible. Right: Example symbolic information available at each frame. At each frame, we record a complete JSON representation of the player state and all objects and artificial agents in the world (x,z,y position in 3D, orientation, etc.). This includes objects outside the player's view recorded in images."
    },
    {
      "section_title": "Standard Episodes",
      "text": "version of Minecraft that provides a complex, colorful 3D world with a pixelated video game aesthetic. Within this world, the agent's goal is to use a carefully selected sequence of actions to turn available resources into a pogo stick. We have recorded the observations of an artificial agent running through hundreds of episodes of this pogo stick construction task, each one producing a sequence of images as well as non-visual symbolic information about the world's state (including all objects, not just those visible from agent's current viewpoint). In some of these episodes, the game has been deliberately modified so that novel types of objects (e.g. TNT explosives, fences, or jukeboxes) are inserted into the world and potentially visible to the agent. Some of these novel objects can impact gameplay (e.g. trees with distinct bark color may yield more resources). Representative images from two \"normal\" episodes and two \"novel\" episodes are shown in Fig. 2, visualizing how novel objects vary in placement and size in complex scenes.\n\nUsing this data, we define three benchmark tasks. First, we have two flavors of novelty detection: (1) using vision (Sec. 4), and (2) using non-visual symbolic or multimodal information (Sec. 5). Next, we consider (3) characterization of novelty via a category discovery task (Sec. 6) where the system must find new objects within complex scenes and organize them within an ever-growing system of classes. We benchmark how state-of-the-art methods perform at each task."
    },
    {
      "section_title": "The contributions of this study are:",
      "text": "1. A scene-focused dataset purpose-built for visual novelty detection. Existing evaluations are too object-focused, artificially creating \"novelty\" by repurposing images designed for object classification. In Sec. 4, we benchmark visual detectors on our new data of complex scenes containing many objects, with novelties often found in the periphery or occluded by other objects when the agent first encounters them. 2. Inclusion of two useful modalities: images and symbolic world-state. In Sec. 5, we show how our data supports visual as well as non-visual symbolic processing to detect novelties. Our dataset can be used to prototype multimodal methods, which are thus far under-explored in the literature. 3. Examination of metrics informed by task-specific costs. While we do report widely used areaunder-curve scores to assess detectors, we contend that reporting only these metrics ignores the need to pick a threshold when the system must produce binary decisions. Threshold selection must be guided by the task-specific costs of false positives versus false negatives in any deployment. 4. Going beyond detection, we assess category discovery. The ability to recognize a previously seen novelty is necessary for an agent to improve with more experience. In Sec. 6, we show how our dataset can be used to inspire improvements in generalized category discovery and share code and pretrained model for generalized category discovery.\n\nUltimately, we hope our work is a step toward a long-term goal of agents that fuse vision with other sensors to adapt to open worlds. Our primary motivation is not one specific real application (like self-driving cars) but rather a general style of problem-solving that we think will become increasingly common and useful: the integration of vision systems for complex scenes within the overall cognitive architectures of a goal-oriented agent to successfully adapt to novelty in 3D open worlds over time.\n\nRationale for focus on a virtual world. We specifically selected this Minecraft-like world because it has several key attributes that are helpful toward our long-term goal. First, it requires long-range planning over time toward a clear goal (pogo stick assembly), not just immediate stimuli-driven responses (e.g. avoid an obstacle in the road). Second, gameplay in this world allows interaction with other agents (both beneficial and adversarial). Third, gameplay requires reasoning about a combinatorial action space as well as 3D movement (vertical or horizontal). Our current benchmark and this research field in general are not at the finish line, only a few steps into a longer journey. We posit that our dataset and evaluation practices are a useful step forward, and the virtual platform we build upon can grow towards this goal."
    },
    {
      "section_title": "Related Work on Datasets for Novelties and Anomalies",
      "text": "Object-focused datasets. Datasets showcasing distinct object types, such as CIFAR-10, Caltech-256, and ImageNet (Deng et al., 2009), originally intended for supervised classification, are often repurposed to assess novelty detection under a k-classes-out paradigm (Ruff et al., 2021). This approach continues to dominate vision research related to novelty detection: The Open World Vision workshop at CVPR 2021 used repurposed images from ImageNet in their open-set recognition challenge (Kumar et al., 2021). Some recent efforts use synthetic perturbations of ImageNet images to assess robustness (Hendrycks & Dietterich, 2019) and separate novelties based on the level of semantic similarity to the training classes (Vaze et al., 2021). Many recent novelty detection methods, such as work by Cheng & Vasconcelos (2021), have yet to be evaluated on scenes, where novelties may not be large, centered or fully visible in the images.\n\nScene-focused datasets. Some recent work has developed datasets that could be used to assess novelty or anomalies in complex visual scenes. The Common Objects in Context (COCO-2014) dataset (Lin et al., 2014) is a widely-used benchmark for scene understanding containing complex images gathered from Flickr, but was not intended to assess novelty detection or discovery. The UBnormal dataset (Acsintoae et al., 2022) contains fixed perspective videos of complex virtual scenes designed to contain anomalous events. Several efforts look at anomalies in scenes gathered from autonomous driving, such as Fishyscapes (Blum et al., 2021) or Road Anomaly (Lis et al., 2019;Chan et al., 2021). NovelCraft is complementary to other scene-focused datasets as some methods may not want to try to overcome the challenge of NovelCraft's severe class imbalance or aim for collaboration with an agent instead of a more typical classification setting. To our knowledge there are no scene-focused datasets that are class-balanced by default, but datasets larger than NovelCraft could more easily be split into subsets to obtain class balancing without reducing performance significantly. A class balanced setting can be more practical for applications where the deployment environment is more controlled, such as robots deployed in a warehouse. UBnormal (Acsintoae et al., 2022) 2019) explore open long-tailed recognition of not-so-common object categories representing the \"long tail\" using a repurposed view of the scene-focused Places dataset (Zhou et al., 2018). Lin et al. (2021) introduce the CLEAR benchmark for continual learning on scene-focused internet images from 2004-2014. Other works explore open-world instance segmentation, either on the COCO dataset (Saito et al., 2022) or on images collected from cars on the road (Chan et al., 2021). Compared to these works, our dataset is distinct in its multimodality and how images are gathered over time by a navigating agent."
    },
    {
      "section_title": "Related Minecraft datasets.",
      "text": "Very recent parallel work has developed the MineDojo benchmark framework (Fan et al., 2022) The virtual world we build upon (with permission from the creators) is PolyCraft2 , a modification of the videogame MineCraft developed by a research team at UT-Dallas (Smaldone et al., 2017;Goss et al., 2023).\n\nPolycraft is multi-purpose software with several applications. We use a particular environment, PolyCraft POGO.\n\nWithin the Polycraft POGO environment, an agent is tasked with exploring a 3-dimensional voxel world via a sequence of actions such as move, break, or craft. Completing the task requires the agent to execute a plan roughly 60 actions long. Moving the agent any distance requires only a single action, so many actions involve the agent interacting with an object or another environment-controlled agent. The agent's goal is to create a pogo stick from resources, such as wood and rubber, that must be gathered from the environment. A solution to the task requires the agent to break trees to acquire wood, craft the wood into planks and sticks, move to a work bench, use the work bench to craft the planks and sticks into a tree tap, place the tree tap adjacent an unbroken tree, use the tree tap to acquire rubber, move to a work bench, and use the work bench to craft rubber, planks, and sticks into a pogo stick. In the event of a novel change in the environment, such as some trees not producing rubber, the agent must be able to change multiple steps in the solution, such as breaking those trees for wood and not using the tree tap on them, to successfully complete the modified task.\n\nAt each step, the agent observes both an image and the environment's symbolic state in JSON text format.\n\nExample images and symbolic information are shown in Fig. 2. Each image is a 256x256 pixel RGB depiction of the agent's current perspective. The JSON text includes positions and names for every object in the environment and every environment-controlled agent, as well as the agent's position and state information (what materials it has collected). This representation is compatible with other possible use cases for agentbased open world novelty detection, where the JSON may represent the environment by listing sensor measurements."
    },
    {
      "section_title": "Data Collection",
      "text": "Num  Our dataset consists of prerendered observations from an agent pursuing the POGO task, with contents summarized in Tab. 2. The data is organized into episodes, an ordered set of data representing the sequence of steps taken by the agent solving the POGO task. Each episode starts with a different initial environment and ends after the agent has completed the task. Each episode is represented by a folder with both an RGB image and a JSON file detailing the state of the environment after each action the agent took. Each image is labeled, indicating if it depicts a non-standard object.\n\nEach episode has a class label indicating how the POGO environment was modified. We collect images under 3 broad scenarios: the standard environment with no modifications, using creator-designed modifications that impact gameplay, and modifying the environment by inserting new objects that are visible but do not impact gameplay (standard agent can still solve the task). Only one modification is applied per episode, so there is never more than one novelty type in an episode. For all collection, we used an autonomous agent following the DIARC cognitive architecture (Scheutz et al., 2019) to solve the pogo-stick assembly task, as described in Muhammad et al. (2021). This agent does not make use of visual images, relying purely on the symbolic world-state available at each step."
    },
    {
      "section_title": "Standard episodes.",
      "text": "Using the standard environment, we collected a base dataset of 99 standard episodes.\n\nOur agent typically produces 45-56 images per episode (5th-95th percentile).\n\nGameplay-altering modifications. The UT-Dallas PolyCraft team created several modifications that would impact gameplay. We selected two that would be visually recognizable: inserting fences that block access to trees and inserting a tree species with distinct bark that can be harvested more efficiently. Harvesting trees is required to build the pogostick, so both modifications modify agent behavior. We collect 80 fence episodes and 85 tree episodes, with our agent producing 29-72 images per episode (5th-95th percentile range)."
    },
    {
      "section_title": "Inserted object modifications.",
      "text": "From a library of possible PolyCraft objects that never appear in the standard task, we selected 51 object types (Tab. A.2) and inserted them into the POGO environment. These objects do not assist or hinder the agent attempting the POGO task, so agent behavior is not changed. We collected at least 10 episodes each, with 32-56 images per episode (5th-95th percentile range).\n\nNovelCraft+: Extra standard episodes. To facilitate models requiring more data we have released NovelCraft+, containing over 2500 episodes and over 100,000 additional frames of standard environment training data. This significantly increases dataset size but results in exaggerated class imbalance between standard and novel images. We use NovelCraft+ to evaluate visual novelty detection (Sec. 4), where class imbalance is best understood and has the least impact. In Sec. 5 and Sec. 6, the original smaller NovelCraft is used for symbolic and multimodal novelty detection and generalized category discovery.\n\nFiltering to identify non-standard images. Within a modified episode, only a few images may actually show non-standard objects. Rendering semantic segmentation maps within the game engine would have been a significant engineering challenge. Therefore we use the locations of agents and objects in the JSON to reconstruct the agent's view within the 3D rendering software Blender (Blender Foundation, 2018). Not all camera parameters were available, so a grid search was used to maximize mean intersection over union for non-standard objects in manually annotated images, achieving 93.0%. For each frame from a modified environment, we can thus render semantic segmentation maps highlighting non-standard objects and record the percentage of pixels containing a non-standard object. We then obtain representative frames from modified episodes by filtering out images with less than 1% non-standard content. This mitigates noise from the rendering, while maintaining challenging examples where non-standard objects are small but clear to a human. Per-frame percentage labels and code for this filtering step are provided in our data release.\n\nSplitting classes into normal/novel and episodes into train/valid/test. We consider 5 total class labels as normal. The \"Standard\" environment with no modifications, the \"Gameplay:Fence\" gameplayaltering modification, and the \"Item:Anvil\", \"Item:Sand\", and \"Item:CoalBlock\" inserted object modifications. This enables methods that repurpose classifiers of multiple normal classes to perform novelty detection."
    },
    {
      "section_title": "Visual Novelty Detection",
      "text": "Here, we describe how we use NovelCraft+ to formulate and evaluate the task of visual novelty detection (Sec. 4.1), which methods we consider (Sec. 4.2), and the results of our experiments (Sec. 4.3). Appendix Sec. E provides experiments assessing how training data size impacts results (NovelCraft+ vs. NovelCraft)."
    },
    {
      "section_title": "Task Description and Evaluation Plan",
      "text": "The goal of our visual novelty detection task is to develop methods that can distinguish whether an image is novel or not. The predefined training set defines 5 normal classes. The goal on test data is to assess if a given image contains content that is not in one of the normal classes.\n\nPerformance metrics. Detectors are evaluated using receiver-operating characteristic (ROC) curves and precision recall (PR) curves. These visualize tradeoffs across possible thresholds that could be applied to the decision score. The ROC curve assesses tradeoffs between true positive rate (TPR, aka recall) and true negative rate (TNR), while PR compares TPR versus positive predictive value (PPV, aka precision). PR curves provide information about imbalanced tasks like novelty detection that ROC curves alone cannot (Saito & Rehmsmeier, 2015). To summarize performance, we report the area under both curves. Higher numbers mean better detectors. We bold the best score and any others within 1.0 percentage point.\n\nMeasuring the area under ROC and PR curves is insufficient for applications that require automated binary decisions of novel or not. Instead of averaging over many thresholds, one specific threshold must be chosen, ideally in a way that respects the task-specific costs of possible mistakes. We consider two different cost regimes when reporting metrics like TPR, TNR, and PPV. First, we prioritize avoiding false negatives, by selecting a threshold achieving 95% TPR. Liang et al. (2018) also evaluated OOD detectors in this regime.\n\nSecond, we avoid too many false positives among all alerts, via a threshold enforcing 80% precision (PPV). As we later show in Tab. 3, the relative ranking of different detectors can change depending on which regime we select. Picking a method based on area-under-the-curve metrics alone might lead to under-performance in the applied regime of interest. This critical distinction may be under appreciated in current evaluation practice.\n\nFrequency of novelty. The relative frequency of the positive class (novel) matters for metrics such as precision. In a typical novel episode, roughly 25% of frames depict novelty. Yet it is difficult to properly score images where very few pixels (0.01-1%) show the novel object. To handle this, we only score images in the filtered subset of the test set, where we are more confident in the label. Then, for metrics where frequencies matter, we take a weighted average with 75% weight on normal images and 25% on novel images."
    },
    {
      "section_title": "Methods for Visual Novelty Detection",
      "text": "We consider several detection methods representing a variety of approaches. We follow Cheng & Vasconcelos (2021) in finetuning a common pretrained VGG-16 network architecture (Simonyan & Zisserman, 2015) for each method utilizing deep classifiers. Each method can access the predefined validation set to tune hyperparameters. For classifiers, grid search maximizes validation accuracy on the normal classes. Novelty detectors then select to maximize validation AUROC. Details for reproducibility (esp. settings for training and hyperparameter grid search) are in the Appendix.\n\nWe first consider NDCC (Cheng & Vasconcelos, 2021)     et al., 2018) creates an adversarial perturbation (Goodfellow et al., 2015), feeds that through the model, and takes the max of the temperature-scaled softmax output as the score.\n\nNext, we consider a denoising Autoencoder (AE) trained without labels to encode a normal image then reconstruct it accurately. We borrow encoder and decoder architectures from Abati et al. (2019). At test time, the reconstruction error is used as a novelty score, which assumes novel classes will be reconstructed less accurately than the normal classes (Richter & Roy, 2017). Our dataset has large images (256x256), so we consider reconstructions that process 32x32 patches (whole image AEs performed worse). After reconstructing each patch, we aggregate the novelty scores via an average over all patches.\n\nWe finally try a One-Class Support Vector Machine (OC-SVM) (Schölkopf et al., 2000). We use the penultimate layer of a finetuned VGG-16 as features and pick an RBF kernel with tuned lengthscale. This model was fit on a subset of 10,000 images due to the method scaling poorly with dataset size."
    },
    {
      "section_title": "Results and Analysis of Visual Novelty Detection Experiments",
      "text": "Tradeoff curves. Using our new dataset, we evaluated all detection methods described in Sec. 4.2 and report their ROC and PR curve in Fig. 3 When false negatives matter most. In Table 3, when enforcing TPR of 95%, the Autoencoder performs best followed by the One-Class SVM and NDCC. However, there is still clearly room for significant improvement as false positives outnumber both true negatives and true positives.\n\nWhen false positives matter most. In Table 3, when enforcing precision of 80%, we find that NDCC, the Autoencoder and Deep Ensemble perform similarly well with respect to their TNR. TPR values are suprisingly low in the 80% precision regime. This suggests an opportunity for models specialized for low false positive regimes (Rath & Hughes, 2022)."
    },
    {
      "section_title": "Symbolic and Multimodal Novelty Detection",
      "text": "While Sec. 4 focused exclusively on visual processing, we will now compare vision-only models to symbolic models that consume the JSON world-state available at each frame, as well as multimodal models that combine visual and symbolic input. To make this evaluation fair and interesting, here we must focus exclusively on gameplay-altering novelties only, so that the symbolic detection task is non-trivial. In this case, it makes more sense to evaluate novelty detection at the episode-level (aggregating across frames observed over time) rather than at the frame-level. Understanding episode-level performance is valuable for assessing agents acting in the same world over time, as we can highlight key properties such as time-delay until correct detection."
    },
    {
      "section_title": "Task Description and Evaluation Plan",
      "text": "Focus on gameplay modification. In our dataset, most modified environments introduce novel objects into the game world that would be trivial to detect symbolically as a never-before-seen key in the JSON description of the world. However, the gameplay novelties by design cause changes in the observable behavior of both agent and environment over time, which a good agent could detect. We thus focus on these gameplay novelties in this task, avoiding trivial easiness by masking any non-standard JSON keys. Because our numerous inserted-object novelties are not appropriate for this task, only a few gameplay novelties remain available. We thus use only the standard episodes from the base NovelCraft dataset for training and validation to maximize diversity in this task's test set, which contains standard episodes and all available gameplay novelty episodes. In addition to the Fence and Tree novelties, we add new Supplier and Thief gameplay novelties, which are exclusive to this task due to only being labeled at the episode level instead of at the frame level. These novelties introduce artificial agents that respectively help and hinder the player, and were released after our initial experiments in Sec. 4 were completed.\n\nEpisode-level predictions. In our proposed benchmark task, models have access to both the RGB image and the JSON symbolic state at each frame in an episode (vision-only methods will ignore the JSON, and symbolic-only methods will ignore the image). The required output is one novelty score for each episode.\n\nFor simplicity, all models predict novelty scores for individual frames, then to score an episode we take the maximum score over all frames. Other strategies for episode-level modeling can be tried in future work.\n\nPerformance metrics. We use the same performance metrics as in Sec. 4.1, evaluated on episodes instead of single frames. While novelty may be easier to detect after experiencing all frames in an episode, failing to detect novelties until the very end of an episode compromises an agent's ability to adapt or take advantage of novelty. We thus also report average detection delay: the number of contiguous frames from the start before a true novelty is detected, averaged across all novelty-containing episodes."
    },
    {
      "section_title": "Methods for Multimodal Novelty Detection",
      "text": "Symbolic approach. We reason that jointly modeling the agent's behavior and its environment is useful, as both may be affected by gameplay novelty. We transform the JSON into a multivariate time-series where each frame t is represented by a vector x t of 24 numeric and 2 categorical variables selected from JSON entries for the player and the world. We model the time series via an K-th order autoregressive model, trained to minimize predictive error over the normal-only training set (Choi et al., 2021;Blázquez-García et al., 2021) Table 4: Episodic novelty detection performance (higher is better for all but delay) on gameplay-only test episodes (Sec. 5). Multimodal models perform better than their components with similar delay to visual models. These per-episode results are not directly comparable with per-frame evaluation in Tab. 3.\n\nMultimodal approach. Our multimodal model is a simple ensemble of the visual and symbolic base detectors. For each base detector, we normalize scores to range from 0 and 1 on the validation set. At test time, the multimodal detector reports the sum of the two normalized scores from visual and symbolic. We chose a patch-based autoencoder for the vision component because it has good performance (top 3 ranking or better in every metric of Tab. 3) and does not require more than one training class (unlike NDCC or ODIN)."
    },
    {
      "section_title": "Results and Analysis: Symbolic and Multimodal Novelty Detection",
      "text": "Tab. 4 reports the performance of visual, symbolic, and multimodal detectors. Overall, this episode-level evaluation intuitively produces higher TPR, TNR, and PPV scores than previous frame-level analysis, due to less sensitivity to frame-level noise. Looking at symbolic performance, 5-frame contexts yield significantly better performance than 2-frame contexts on ROC based metrics, suggesting medium-range temporal modeling matters for our benchmark. Finally, the multimodal ensemble nicely blends the best of both visual and symbolic models to improve performance with delay similar to the visual model across both cost regimes. Future methods have plenty of room to improve (human performance would be close to 100% AUROC).\n\n6 Generalized Category Discovery"
    },
    {
      "section_title": "Task Description and Evaluation Plan",
      "text": "Both generalized category discovery (GCD) (Vaze et al., 2022) and novel category discovery (NCD) (Zhao & Han, 2021;Han et al., 2020) consider the problem of assigning labels to a set of unlabeled data by utilizing a set of related labeled data. NCD assumes this unlabeled data contains only new classes not seen in the labeled data, while GCD allows labeled classes and new classes to appear in the unlabeled set. Unlike novelty detection, both the labeled and unlabeled data are available during training. During testing, clustering accuracy is used to evaluate the labels assigned to the unlabeled set. We additionally report clustering accuracy on the subset of classes seen in the labeled set as well as the new classes.\n\nTo try GCD on our base NovelCraft data, we utilize our training split as the labeled set and our validation split as the unlabeled set. This results in 5 classes in the labeled set and 10 classes in the unlabeled set, which includes new examples of the labeled set classes. In our evaluation we assume prior knowledge of the number of new classes in the unlabeled set, but future evaluations may utilize the method in Vaze et al. (2022) to select the number of new classes. We leave GCD assessment on our larger NovelCraft+ to future work, as methods for GCD under severe class imbalance have not been well explored to our knowledge."
    },
    {
      "section_title": "Methods for Category Discovery",
      "text": "GCD SSKM. Following Vaze et al. (2022), we use a ViT-B-16 (Dosovitskiy et al., 2020) vision transformer pretrained with DINO (Caron et al., 2021) self-supervision on unlabeled ImageNet then fine-tuned with the GCD contrastive loss on NovelCraft. The contrastive loss proposed by Vaze et al. (2022) has an unlabeled term and a labeled term, with the unlabeled term applied to all data and the labeled term applied only to labeled data. The unlabeled term encourages two random augmentations of the same image to be mapped close to each other in embedding space, penalizing being close to different images in the same mini-batch. The labeled term encourages each pair of images that share a class to be mapped close to each other in embedding space, penalizing being close to images from other classes.\n\nClass-balanced sampling is utilized on the labeled set to account for class imbalance. The learned features from this model are then fed into semi-supervised k-means (SSKM) (Vaze et al., 2022), which learns cluster assignments for all examples that respect known labels from the labeled set."
    },
    {
      "section_title": "DINO SSKM baseline.",
      "text": "To examine the effectiveness of GCD's fine-tuning, we compare to a semi-supervised k-means baseline that uses pretrained DINO features. We chose this instead of unsupervised k-means as in Vaze et al. (2022), so that both methods utilize the labeled set examples.\n\nSemi-supervised Gaussian mixture. To examine the effectiveness of SSKM's clustering, we compare to a semi-supervised Gaussian mixture model (SSGMM). SSGMM is a new extension of Gaussian mixture models that applies the semi-supervision method of SSKM (Vaze et al., 2022) to learn cluster assignments that respect known labels from the labeled set. SSGMM should be more flexible as it allows soft assignment (multiple clusters can take partial responsibility for a given data example), can learn the frequencies of each cluster, and can learn the appropriate scaling of a cluster-specific spherical covariance. In contrast, k-means assumes hard assignment (each data example matched to exactly one cluster), assumes uniform cluster frequencies, and represents the limiting behavior when all clusters share a single spherical covariance whose scale diminishes to zero. We use a modified version of expectation maximization (Dempster et al., 1977) to fit this GMM in semi-supervised fashion. Semi-supervision is introduced by modifying the expectation step so that each labeled example is assigned to the correct cluster corresponding to its label as in SSKM, with no responsibility assigned to incorrect clusters."
    },
    {
      "section_title": "Results and Analysis",
      "text": "Clustering   Improving GCD to address class imbalance in unlabeled data will be challenging. Balancing the labeled examples used by semi-supervised k-means risks increasing the splitting of more frequent classes, such as the predicted I3 and I4 classes consisting of a subset of the most frequent classes. Separating new classes from the standard class likely requires specialized modifications to the loss, as all unlabeled data resemble standard class images to some extent."
    },
    {
      "section_title": "Discussion and Outlook",
      "text": "We hope our NovelCraft dataset inspires new methods development on novelty detection and characterization, enabling agents to explore open worlds."
    },
    {
      "section_title": "Limitations.",
      "text": "Our benchmark focuses on one specific open world. We wish to avoid overclaiming about generality (Raji et al., 2021). Many other open worlds are possible. The kinds of visual properties we call novel emphasize object types. We could instead have varied counts, sizes, spatial relationships, or co-occurrance patterns between objects. We include only a few gameplay modifications due to the cost of creating these. More diverse gameplay novelties that are not trivial to detect symbolically would be worth future investment.\n\nOur multimodal experiments are limited to our specific choice for JSON data. JSON can encode less precise sensory inputs which are worth further exploration.\n\nFuture Directions. We can foresee several promising lines of work building on our NovelCraft data:\n\nMultimodal novelty detection. A recent review suggests that multimodal novelty detection is \"a largely unexplored research area\" (Pang et al., 2022). Our dataset provides two complementary modalities, symbolic and visual, that could both be leveraged by future methods.\n\nContinual learning. Our new dataset is naturally episodic in nature. Future work could use a continual learning paradigm (Lin et al., 2021) instead of \"train once, then deploy\", which is far more realistic for open-world learning. Our discovery task in Sec. 6 could be a promising first step.\n\nFrom vision to action. Our dataset is collected by a real artificial agent interacting in a virtual world. Future work could use visual novelty processing to inform how the agent selects actions to solve its task. Active perception is especially promising: novel objects may or may not look novel from all perspectives; an agent may need to actively obtain distinct perspectives to determine a new object's properties and how it could be used in service of the agent's goal."
    },
    {
      "section_title": "Broader Impact Statement",
      "text": "We have designed our dataset with the goal of positive benefit to society. We think novelty detection and characterization can help build agents that recognize when a model is not suitable (and might even be harmful) and adapt accordingly. We hope that our findings on the difficulties of adapting models from research settings to applications encourages users to always inspect a model's potential impacts. We recognize that there is a potential for misuse, as with all classifiers and especially visual ones, if the classifier is used to discriminate by gender, ethnicity, and other protected categories. Ultimately, we hope that the potential of novelty detection to preempt accidental misuse will outweigh the impacts of purposeful misuse of these techniques.\n\nSplits is provided with the dataset code as \"splits.csv\" and lists the splits for the training set classes, divided by episode. Which classes are assigned to novel validation and novel test are listed in \"data_const.py\"."
    },
    {
      "section_title": "C.1 Visual Novelty Detection",
      "text": "Except for NDCC and One-Class SVM, all methods are implemented in PyTorch (Paszke et al., 2019) and are trained and tested on a GPU (NVIDIA TITAN X Pascal). NDCC was trained and tested on a NVIDIA RTX 2080 GPU, but note that fewer training epochs is the primary reason for reduced training time. The One-Class SVM is implemented using the SciKit-Learn software package (Pedregosa et al., 2011) and is trained and tested on a CPU. See"
    },
    {
      "section_title": "C.1.1 VGG-16 Backbone",
      "text": "A VGG-16 (Simonyan & Zisserman, 2015) is used as a backbone for deep ensemble and ODIN and as a feature extractor for the one-class SVM.\n\nArchitecture. The VGG-16 model of the PyTorch framework (Paszke et al., 2019) pretrained on ImageNet (Deng et al., 2009) is adapted and fine tuned to classify the normal classes. The pretrained classification head is replaced with a randomly initialized 5 output fully connected layer classication head.\n\nTraining Details.We optimize the parameters using the cross-entropy loss function and the Adam optimizer (Kingma & Ba, 2014) Training Details. We train the predictive models using the Adam optimizer with a learning rate of 10 −4 for 50 epochs with early stopping. We use default parameters of 0 weight decay and β 1,2 = (0.9, 0.999) in the optimizer. For numerical features, the loss function optimized is MSE whereas for categorical features we optimize Categorical Cross-Entropy.\n\nHyperparameter Tuning. For our predictive MLP models we fine-tune the number of hidden layers, and a single value for all hidden dimensions. We perform grid search over 1 to 3 hidden layers with step size 1 with hidden dimensions varying between 16 to 64 with step size 16. We perform model selection based on the validation predictive loss. For all three settings of the context size, the final hyperparameters are 2 hidden layers with hidden dimension of 64. We also use a default dropout rate of 0.2 for all models and ReLU activations in all layers except the last.\n\nAnomaly Scores. To calculate anomaly scores for a single time-step we take the maximum over the predictive loss of all variables in the vector."
    },
    {
      "section_title": "C.3 Generalized Category Discovery",
      "text": "Architecture. Following Vaze et al. (2022), we use a ViT-B-16 (Dosovitskiy et al., 2020) vision transformer pretrained with DINO (Caron et al., 2021) self-supervision on unlabeled ImageNet then fine-tuned on NovelCraft.\n\nTraining Details. Fine-tuning is done with the GCD contrastive loss with supervised loss weight 0.35. Training consists of 200 epochs of SGD optimization with a cosine annealed learning rate ending at the initial learning rate times 0.01.\n\nHyperparameter Tuning. Supervised loss weights of 0 and 1 were tested to verify that the combination of losses was increasing performance. Initial learning rates 1, 0.1, 0.01, and 0.001 were tested with 0.1 found to best minimize training loss, reaching −1.21 mean loss."
    },
    {
      "section_title": "D Further Comparisons to Related Work",
      "text": "D.1 Further Discussion and Analysis of Related Datasets.\n\nSurveillance-focused datasets. Work on anomaly detection in complex scenes has often been motivated by surveillance applications (Ramachandra et al., 2022), such as the UCSD Anomaly Detection dataset (Li et al., 2014). The yearly PETS challenges (Patino et al., 2017) have offered open datasets with anomalies such as abandoned baggage. Unlike these fixed camera datasets, our dataset uses a dynamic, egocentric camera and enjoys fewer ethical concerns than surveillance.\n\nComparison to scene-focused datasets. In Tab. D.1 (next page), we provide a comprehensive comparison of available datasets focused on scene-focused visual tasks that seem especially relevant for novelty/anomaly detection. We thank anonymous reviewers for bringing some of these to our attention.\n\nTo recap the major takeaway messages from this"
    },
    {
      "section_title": "D.2 Performance Comparion Across Datasets for Visual Novelty Detection",
      "text": "See Table D.2 for visual novelty detection results on other datasets. Our benchmark is more challenging than many existing visual novelty detection evaluations, with only the experiments by Cheng & Vasconcelos (2021) on CUB-200-2010and Abati et al. (2019) on CIFAR10 getting similar AUROC metrics. We also note that varying model backbones, evaluation methodologies, datasets, and the use of only AUROC as a metric hinders model comparison. We hope to enable future comparisons between models by providing a dataset with defined visual novelty evaluation methodology, evaluations of a variety of methods with the same backbone, and more metrics for detailed model comparisons."
    },
    {
      "section_title": "Method",
      "text": "Backbone"
    }
  ]
}