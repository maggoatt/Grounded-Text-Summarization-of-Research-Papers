{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maggoatt/Grounded-Text-Summarization-of-Research-Papers/blob/main/Summarization_Model_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1AFc3_cS0cW"
      },
      "source": [
        "## Summarization Model Workflow\n",
        "\n",
        "- Baseline: TextRank\n",
        "- Advanced: Facebook BART (Large-CNN)\n",
        "High-level pipeline:\n",
        "1. Take in the selected paper (i.e. from ```streamlit``` file)\n",
        "2. Sliding window (i.e. 1k tokens) to chunk paper, take note of the section titles per chunk\n",
        "3. Generate summaries per chunk per model and stitch together\n",
        "\n",
        "### Citations/references:\n",
        "\n",
        "1. Workflow to implement TextRank: \n",
        "\n",
        "Adapted from: ERRAJI, Yassine (June 19 2025). [\"Understanding TextRank: A Deep Dive into Graph-Based Text Summarization and Keyword Extraction\"](https://medium.com/@yassineerraji/understanding-textrank-a-deep-dive-into-graph-based-text-summarization-and-keyword-extraction-905d1fb5d266).\n",
        "Medium Article.\n",
        "\n",
        "2. Workflow to implement Facebook BART:\n",
        "\n",
        "Adapted from: Lewis, Mike _et al._ (Accessed February 2026). [\"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\"](https://huggingface.co/facebook/bart-large-cnn).\n",
        "Hugging Face Documentation.\n",
        "\n",
        "Adapted from: baksapeter (April 11, 2025). [\"Maximum number of input tokens\"](https://huggingface.co/facebook/bart-large-cnn/discussions/83). Hugging Face Discussion.\n",
        "\n",
        "3. Misc. syntax: scikit-learn documentation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample S2ORC JSON formatting\n",
        "```{\n",
        "  \"paper_id\": \"553755490\",\n",
        "  \"header\": {\n",
        "    \"title\": \"Example Title of a Scientific Paper\",\n",
        "    \"authors\": [\n",
        "      {\"first\": \"John\", \"middle\": [\"A\"], \"last\": \"Doe\", \"affiliation\": {\"name\": \"University of Example\"}},\n",
        "      {\"first\": \"Jane\", \"middle\": [], \"last\": \"Smith\", \"affiliation\": {\"name\": \"Example Institute\"}}\n",
        "    ]\n",
        "  },\n",
        "  \"abstract\": [\n",
        "    {\n",
        "      \"text\": \"This is an example of the abstract text in the S2ORC corpus.\",\n",
        "      \"cite_spans\": [],\n",
        "      \"ref_spans\": []\n",
        "    }\n",
        "  ],\n",
        "  \"body_text\": [\n",
        "    {\n",
        "      \"section\": \"Introduction\",\n",
        "      \"text\": \"This paragraph represents the body text. It can contain citations like (Doe et al., 2020) and references to figures or tables.\",\n",
        "      \"cite_spans\": [\n",
        "        {\n",
        "          \"start\": 50,\n",
        "          \"end\": 65,\n",
        "          \"text\": \"(Doe et al., 2020)\",\n",
        "          \"ref_id\": \"BIBREF0\"\n",
        "        }\n",
        "      ],\n",
        "      \"ref_spans\": []\n",
        "    },\n",
        "    {\n",
        "      \"section\": \"Methodology\",\n",
        "      \"text\": \"Details of the method, including reference to Figure 1.\",\n",
        "      \"cite_spans\": [],\n",
        "      \"ref_spans\": [\n",
        "        {\n",
        "          \"start\": 40,\n",
        "          \"end\": 48,\n",
        "          \"text\": \"Figure 1\",\n",
        "          \"ref_id\": \"FIGREF0\"\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ],\n",
        "  \"bib_entries\": {\n",
        "    \"BIBREF0\": {\n",
        "      \"title\": \"A seminal paper on the subject\",\n",
        "      \"authors\": [\"Doe\", \"J.\", \"Smith\", \"J.\"],\n",
        "      \"year\": 2020,\n",
        "      \"venue\": \"Journal of Examples\",\n",
        "      \"volume\": \"1\",\n",
        "      \"pages\": \"100-110\"\n",
        "    }\n",
        "  },\n",
        "  \"ref_entries\": {\n",
        "    \"FIGREF0\": {\n",
        "      \"num\": \"1\",\n",
        "      \"type\": \"figure\",\n",
        "      \"text\": \"A description of Figure 1\"\n",
        "    }\n",
        "  }\n",
        "}```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.8.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
            "Collecting networkx\n",
            "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-5.0.0-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting numpy>=1.24.1 (from scikit-learn)\n",
            "  Downloading numpy-2.4.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (6.6 kB)\n",
            "Collecting scipy>=1.10.0 (from scikit-learn)\n",
            "  Downloading scipy-1.17.0-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
            "Collecting joblib>=1.3.0 (from scikit-learn)\n",
            "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
            "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting filelock (from transformers)\n",
            "  Using cached filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting huggingface-hub<2.0,>=1.3.0 (from transformers)\n",
            "  Downloading huggingface_hub-1.3.5-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from transformers) (26.0)\n",
            "Collecting pyyaml>=5.1 (from transformers)\n",
            "  Downloading pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Downloading regex-2026.1.15-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Downloading tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
            "Collecting typer-slim (from transformers)\n",
            "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
            "Collecting tqdm>=4.27 (from transformers)\n",
            "  Downloading tqdm-4.67.2-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
            "  Using cached fsspec-2026.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
            "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting shellingham (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
            "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting typing-extensions>=4.1.0 (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
            "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting anyio (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
            "  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting certifi (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
            "  Using cached certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
            "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting idna (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
            "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers)\n",
            "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting click>=8.0.0 (from typer-slim->transformers)\n",
            "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading scikit_learn-1.8.0-cp313-cp313-macosx_12_0_arm64.whl (8.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
            "Downloading transformers-5.0.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-1.3.5-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m468.6 kB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm-:--:--\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Downloading tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached fsspec-2026.1.0-py3-none-any.whl (201 kB)\n",
            "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Using cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
            "Downloading numpy-2.4.2-cp313-cp313-macosx_14_0_arm64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl (173 kB)\n",
            "Downloading regex-2026.1.15-cp313-cp313-macosx_11_0_arm64.whl (288 kB)\n",
            "Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
            "Downloading scipy-1.17.0-cp313-cp313-macosx_14_0_arm64.whl (20.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading tqdm-4.67.2-py3-none-any.whl (78 kB)\n",
            "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Downloading anyio-4.12.1-py3-none-any.whl (113 kB)\n",
            "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
            "Using cached certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
            "Using cached filelock-3.20.3-py3-none-any.whl (16 kB)\n",
            "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
            "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
            "Installing collected packages: typing-extensions, tqdm, threadpoolctl, shellingham, safetensors, regex, pyyaml, numpy, networkx, joblib, idna, hf-xet, h11, fsspec, filelock, click, certifi, typer-slim, scipy, httpcore, anyio, scikit-learn, httpx, huggingface-hub, tokenizers, transformers\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/26\u001b[0m [transformers][0m [transformers]ub]\n",
            "\u001b[1A\u001b[2KSuccessfully installed anyio-4.12.1 certifi-2026.1.4 click-8.3.1 filelock-3.20.3 fsspec-2026.1.0 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-1.3.5 idna-3.11 joblib-1.5.3 networkx-3.6.1 numpy-2.4.2 pyyaml-6.0.3 regex-2026.1.15 safetensors-0.7.0 scikit-learn-1.8.0 scipy-1.17.0 shellingham-1.5.4 threadpoolctl-3.6.0 tokenizers-0.22.2 tqdm-4.67.2 transformers-5.0.0 typer-slim-0.21.1 typing-extensions-4.15.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# installing dependencies\n",
        "\n",
        "%pip install scikit-learn networkx transformers # for TextRank (networkx) and BART (transformers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "# TextRank\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "import json\n",
        "\n",
        "# BART\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TextRank Pipeline:\n",
        "1. Extract + concatenate text from selected paper (to be referenced from JSON object created by UI/API request)\n",
        "2. Tokenize extracted + concatenated text\n",
        "3. Create similarity graph of tokens\n",
        "4. Run PageRank\n",
        "5. Rank by top-k and output final summary\n",
        "\n",
        "Additionally, preserve which section the sentence originated from (for later analysis/retrieval purposes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# current method: concatenate all paragraphs from just the body section together. no splitting by section\n",
        "\n",
        "k = 5 # summary sentence length\n",
        "\n",
        "paper = json.loads(...) # load the selected paper's json file\n",
        "body_text = []\n",
        "section_map = {} # preserving sentences' og section\n",
        "\n",
        "for section in paper[\"body_text\"]: # (1) extract and concatenate text from selected paper\n",
        "    section_title = section[\"section\"]\n",
        "    sentences = [s.strip() for s in section[\"text\"].replace('?', '.').replace('!', '.').split('.')] # splitting sentences by punc, then strip any leading whitespace\n",
        "   \n",
        "    for sentence in sentences:\n",
        "        if sentence:\n",
        "            section_map[len(body_text)] = section_title  # track section of sentence based on index of sentence\n",
        "            body_text.append(sentence)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(body_text) # (2) grab doc-term mtx, treating each sentence as a document in body_text corpus\n",
        "similarity_mtx = cosine_similarity(X) # (3) cosine sim on sentences based on word importance\n",
        "graph = nx.from_numpy_array(similarity_mtx)\n",
        "\n",
        "scores = nx.pagerank(graph) # (4) score sentences via PageRank\n",
        "\n",
        "ranked = sorted(((scores[i], s, section_map[i]) for i, s in enumerate(body_text)), reverse=True) # sentences and section name ranked by highest scores\n",
        "\n",
        "summary = \". \".join([s for _, s, _ in ranked[:k]]) + \".\"\n",
        "print(summary) # (5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Facebook BART Pipeline:\n",
        "1. Create summarization pipeline, specifying Facebook BART (large-CNN model)\n",
        "2. Extract + concatenate text from selected paper\n",
        "3. Check if token count exceeds Facebook BART max input token count (1024)\n",
        "4. If token count > 1024, implement sliding window. Else, summarize entire input\n",
        "5. Output the final summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model=\"facebook/bart-large-cnn\" # (1)\n",
        "\n",
        "full_body_text = \". \".join(body_text) # (2) turn the list of sentences into string\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "tokens = tokenizer.encode(full_body_text, truncation=False)\n",
        "token_count = len(tokens)\n",
        "max_token_count = tokenizer.model_max_length\n",
        "\n",
        "print(f\"total tokens: {token_count}\\nmax allowed tokens: {max_token_count}\")\n",
        "if token_count > max_token_count: # (3)\n",
        "    # (4) TODO: sliding window technique\n",
        "    ...\n",
        "else: # can pass in text from entire body of paper\n",
        "    summarizer = pipeline(\"summarization\", model=model)\n",
        "    summary = summarizer(full_body_text, max_length=k*20, min_length=k*10, do_sample=False) # sentences are usually 15-20 words long\n",
        "\n",
        "print(summary[0][\"summary_text\"]) # (5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPvgGZfwLldGEkhAgBwkRCJ",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
