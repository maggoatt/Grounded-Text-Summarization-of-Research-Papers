{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maggoatt/Grounded-Text-Summarization-of-Research-Papers/blob/main/Summarization_Model_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1AFc3_cS0cW"
      },
      "source": [
        "## Summarization Model Workflow\n",
        "\n",
        "- Baseline: TextRank\n",
        "- Advanced: Facebook BART (Large-CNN)\n",
        "High-level pipeline:\n",
        "1. Take in the selected paper (i.e. from ```streamlit``` file)\n",
        "2. Sliding window (i.e. 1k tokens) to chunk paper, take note of the section titles per chunk\n",
        "3. Generate summaries per chunk per model and stitch together\n",
        "\n",
        "### Citations/references:\n",
        "\n",
        "1. Workflow to implement TextRank: \n",
        "\n",
        "Adapted from: ERRAJI, Yassine (June 19 2025). [\"Understanding TextRank: A Deep Dive into Graph-Based Text Summarization and Keyword Extraction\"](https://medium.com/@yassineerraji/understanding-textrank-a-deep-dive-into-graph-based-text-summarization-and-keyword-extraction-905d1fb5d266).\n",
        "Medium Article.\n",
        "\n",
        "2. Workflow to implement Facebook BART:\n",
        "\n",
        "Adapted from: Lewis, Mike _et al._ (Accessed February 2026). [\"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\"](https://huggingface.co/facebook/bart-large-cnn).\n",
        "Hugging Face Documentation.\n",
        "\n",
        "Adapted from: baksapeter (April 11, 2025). [\"Maximum number of input tokens\"](https://huggingface.co/facebook/bart-large-cnn/discussions/83). Hugging Face Discussion.\n",
        "\n",
        "3. Misc. syntax: scikit-learn documentation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.13/site-packages (1.8.0)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.13/site-packages (3.6.1)\n",
            "Requirement already satisfied: transformers in ./.venv/lib/python3.13/site-packages (5.0.0)\n",
            "Requirement already satisfied: numpy>=1.24.1 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (2.4.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.3.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in ./.venv/lib/python3.13/site-packages (from transformers) (1.3.5)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2026.1.15)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.13/site-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in ./.venv/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.13/site-packages (from transformers) (4.67.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in ./.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: anyio in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.13/site-packages (from typer-slim->transformers) (8.3.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# installing dependencies\n",
        "\n",
        "%pip install scikit-learn networkx transformers # for TextRank (networkx) and BART (transformers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "# TextRank\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "import json\n",
        "\n",
        "# BART\n",
        "import torch\n",
        "from transformers import AutoTokenizer, BartForConditionalGeneration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TextRank Pipeline:\n",
        "1. Extract + concatenate text from selected paper (to be referenced from JSON object created by UI/API request)\n",
        "2. Tokenize extracted + concatenated text\n",
        "3. Create similarity graph of tokens\n",
        "4. Run PageRank\n",
        "5. Rank by top-k and output final summary\n",
        "\n",
        "Additionally, preserve which section the sentence originated from (for later analysis/retrieval purposes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extracting content from paper\n",
        "test_path = \"data/249953535.json\"\n",
        "k = 5 # summary sentence length\n",
        "\n",
        "with open(test_path, 'r', encoding='utf-8') as f:\n",
        "    paper = json.load(f) # load the selected paper's json file\n",
        "    \n",
        "cid = paper[\"corpusid\"]\n",
        "body_text = []\n",
        "section_map = {} # preserving sentences' og section\n",
        "\n",
        "# current method: concatenate all paragraphs from just the body section together. no splitting by section\n",
        "for section in paper[\"sections\"]: # (1) extract and concatenate text from selected paper\n",
        "    section_title = section[\"section_title\"]\n",
        "    sentences = [s.strip() for s in section[\"text\"].replace('?', '.').replace('!', '.').split('.')] # splitting sentences by punc, then strip any leading whitespace\n",
        "   \n",
        "    for sentence in sentences:\n",
        "        if sentence:\n",
        "            section_map[len(body_text)] = section_title  # track section of sentence based on index of sentence\n",
        "            body_text.append(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We thus use only the standard episodes from the base NovelCraft dataset for training and validation to maximize diversity in this task's test set, which contains standard episodes and all available gameplay novelty episodes. In our evaluation we assume prior knowledge of the number of new classes in the unlabeled set, but future evaluations may utilize the method in Vaze et al. To handle this, we only score images in the filtered subset of the test set, where we are more confident in the label. In the event of a novel change in the environment, such as some trees not producing rubber, the agent must be able to change multiple steps in the solution, such as breaking those trees for wood and not using the tree tap on them, to successfully complete the modified task. In addition to the Fence and Tree novelties, we add new Supplier and Thief gameplay novelties, which are exclusive to this task due to only being labeled at the episode level instead of at the frame level.\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(body_text) # (2) grab doc-term mtx, treating each sentence as a document in body_text corpus\n",
        "similarity_mtx = cosine_similarity(X) # (3) cosine sim on sentences based on word importance\n",
        "graph = nx.from_numpy_array(similarity_mtx)\n",
        "\n",
        "scores = nx.pagerank(graph) # (4) score sentences via PageRank\n",
        "\n",
        "ranked = sorted(((scores[i], s, section_map[i]) for i, s in enumerate(body_text)), reverse=True) # sentences and section name ranked by highest scores\n",
        "\n",
        "summary = \". \".join([s for _, s, _ in ranked[:k]]) + \".\"\n",
        "print(summary) # (5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# export summary to .txt document for benchmarking analysis:\n",
        "\n",
        "file_path = f\"./summaries/{cid}_textrank_summary.txt\"\n",
        "\n",
        "with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Facebook BART Pipeline:\n",
        "1. Create summarization pipeline, specifying Facebook BART (large-CNN model)\n",
        "2. Extract + concatenate text from selected paper\n",
        "3. Check if token count exceeds Facebook BART max input token count (1024)\n",
        "4. If token count > 1024, implement sliding window. Else, summarize entire input\n",
        "5. Output the final summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 511/511 [00:00<00:00, 1464.05it/s, Materializing param=model.encoder.layers.11.self_attn_layer_norm.weight]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total tokens: 8272\n",
            "max allowed tokens: 1024\n",
            "\n",
            "--- Summarizing 29 sections ---\n",
            "  section 1/29: 708 tokens -> 60 tokens\n",
            "  section 2/29: 319 tokens -> 77 tokens\n",
            "  section 3/29: 553 tokens -> 52 tokens\n",
            "  section 4/29: 617 tokens -> 47 tokens\n",
            "  section 5/29: 474 tokens -> 56 tokens\n",
            "  section 6/29: 274 tokens -> 74 tokens\n",
            "  section 7/29: 140 tokens -> 46 tokens\n",
            "  section 8/29: 506 tokens -> 68 tokens\n",
            "  section 9/29: 76 tokens -> 45 tokens\n",
            "  section 10/29: 522 tokens -> 65 tokens\n",
            "  section 11/29: 399 tokens -> 55 tokens\n",
            "  section 12/29: 170 tokens -> 62 tokens\n",
            "  section 13/29: 152 tokens -> 69 tokens\n",
            "  section 14/29: 437 tokens -> 51 tokens\n",
            "  section 15/29: 304 tokens -> 48 tokens\n",
            "  section 16/29: 152 tokens -> 66 tokens\n",
            "  section 17/29: 300 tokens -> 64 tokens\n",
            "  section 18/29: 265 tokens -> 86 tokens\n",
            "  section 19/29: 318 tokens -> 60 tokens\n",
            "  section 20/29: 99 tokens -> 54 tokens\n",
            "  section 21/29: 25 tokens -> 25 tokens\n",
            "  section 22/29: 366 tokens -> 50 tokens\n",
            "  section 23/29: 187 tokens -> 72 tokens\n",
            "  section 24/29: 111 tokens -> 95 tokens\n",
            "  section 25/29: 382 tokens -> 66 tokens\n",
            "  section 26/29: 194 tokens -> 78 tokens\n",
            "  section 27/29: 180 tokens -> 41 tokens\n",
            "  section 28/29: 141 tokens -> 34 tokens\n",
            "  section 29/29: 4 tokens -> 28 tokens\n",
            "\n",
            "combined section summaries: 1637 tokens\n",
            "  still > 1024 tokens, entering reduction loop...\n",
            "\n",
            "--- Round 2: summarizing 2 chunks ---\n",
            "  chunk 1/2: 972 tokens -> 70 tokens\n",
            "  chunk 2/2: 667 tokens -> 87 tokens\n",
            "  combined result: 155 tokens\n",
            "  fits within 1024 tokens, concatenating summaries\n",
            "\n",
            "================================================================================\n",
            "FINAL SUMMARY:\n",
            "================================================================================\n",
            "Existing evaluations are too object-focused, artificially creating \"novelty\" by repurposing images designed for object classification. Our dataset can be used to prototype multimodal methods, which are thus far under-explored in the literature. Autoencoder performs best followed by the One-Class SVM and NDCC. GCD SSKM. We use a ViT-B-16 vision transformer pretrained with DINO (Caron et al., 2021) self-supervision on unlabeled ImageNet. Fine-tuning is done with the GCD contrastive loss with supervised loss weight 0.35. We train the predictive models using the Adam optimizer with a learning rate of 10 −4 for 50 epochs.\n"
          ]
        }
      ],
      "source": [
        "model_name = \"facebook/bart-large-cnn\" # (1)\n",
        "\n",
        "full_body_text = \". \".join(body_text) # (2) turn the list of sentences into string\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "max_token_count = 1024 # BART's actual positional encoding limit\n",
        "\n",
        "def summarize(text, max_new_tokens=300, min_new_tokens=20):\n",
        "    \"\"\"Summarize a single chunk of text using BART (input auto-truncated to 1024 tokens).\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_token_count, truncation=True)\n",
        "    summary_ids = bart_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        min_new_tokens=min_new_tokens,\n",
        "        num_beams=4,\n",
        "        length_penalty=2.0,\n",
        "        forced_bos_token_id=0\n",
        "    )\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "def get_token_count(text):\n",
        "    return len(tokenizer.encode(text, truncation=False))\n",
        "\n",
        "def reduce_summaries(texts, round_num=1):\n",
        "    \"\"\"\n",
        "    Recursively summarize until the combined text fits within 1024 tokens.\n",
        "    \n",
        "    1. Summarize each chunk individually\n",
        "    2. Concatenate the summaries\n",
        "    3. If still > 1024 tokens, group into chunks and repeat\n",
        "    4. Once <= 1024 tokens, produce the final summary\n",
        "    \"\"\"\n",
        "    print(f\"--- Round {round_num}: summarizing {len(texts)} chunks ---\")\n",
        "    \n",
        "    chunk_summaries = []\n",
        "    for i, text in enumerate(texts):\n",
        "        tc = get_token_count(text)\n",
        "        summary = summarize(text)\n",
        "        print(f\"  chunk {i+1}/{len(texts)}: {tc} tokens -> {get_token_count(summary)} tokens\")\n",
        "        chunk_summaries.append(summary)\n",
        "    \n",
        "    # combine all summaries into one text\n",
        "    combined = \" \".join(chunk_summaries)\n",
        "    combined_tokens = get_token_count(combined)\n",
        "    print(f\"  combined result: {combined_tokens} tokens\")\n",
        "    \n",
        "    if combined_tokens <= max_token_count:\n",
        "        # fits within limit — concatenate and return as-is\n",
        "        print(f\"  fits within {max_token_count} tokens, concatenating summaries\")\n",
        "        return combined\n",
        "    else:\n",
        "        # still too long — group summaries into 1024-token chunks and recurse\n",
        "        print(f\"  still > {max_token_count} tokens, splitting again...\\n\")\n",
        "        groups = []\n",
        "        current_group = []\n",
        "        current_tokens = 0\n",
        "        for s in chunk_summaries:\n",
        "            s_tokens = get_token_count(s)\n",
        "            if current_tokens + s_tokens > max_token_count and current_group:\n",
        "                groups.append(\" \".join(current_group))\n",
        "                current_group = [s]\n",
        "                current_tokens = s_tokens\n",
        "            else:\n",
        "                current_group.append(s)\n",
        "                current_tokens += s_tokens\n",
        "        if current_group:\n",
        "            groups.append(\" \".join(current_group))\n",
        "        \n",
        "        return reduce_summaries(groups, round_num + 1)\n",
        "\n",
        "# --- run the pipeline ---\n",
        "token_count = get_token_count(full_body_text)\n",
        "print(f\"total tokens: {token_count}\\nmax allowed tokens: {max_token_count}\\n\")\n",
        "\n",
        "if token_count > max_token_count:\n",
        "    # step 1: summarize each section individually (preserves 1:1 mapping with section titles)\n",
        "    section_texts = [section[\"text\"] for section in paper[\"sections\"]]\n",
        "    summaries = []\n",
        "    print(f\"--- Summarizing {len(section_texts)} sections ---\")\n",
        "    for i, text in enumerate(section_texts):\n",
        "        tc = get_token_count(text)\n",
        "        s = summarize(text)\n",
        "        print(f\"  section {i+1}/{len(section_texts)}: {tc} tokens -> {get_token_count(s)} tokens\")\n",
        "        summaries.append(s)\n",
        "    \n",
        "    # step 2: combine section summaries and reduce until it fits in 1024 tokens\n",
        "    combined = \" \".join(summaries)\n",
        "    combined_tokens = get_token_count(combined)\n",
        "    print(f\"\\ncombined section summaries: {combined_tokens} tokens\")\n",
        "    \n",
        "    if combined_tokens <= max_token_count:\n",
        "        # already fits — concatenate and use as final summary\n",
        "        print(f\"  fits within {max_token_count} tokens, concatenating summaries\")\n",
        "        summary_text = combined\n",
        "    else:\n",
        "        # need more reduction rounds\n",
        "        print(f\"  still > {max_token_count} tokens, entering reduction loop...\\n\")\n",
        "        groups = []\n",
        "        current_group = []\n",
        "        current_tokens = 0\n",
        "        for s in summaries:\n",
        "            s_tokens = get_token_count(s)\n",
        "            if current_tokens + s_tokens > max_token_count and current_group:\n",
        "                groups.append(\" \".join(current_group))\n",
        "                current_group = [s]\n",
        "                current_tokens = s_tokens\n",
        "            else:\n",
        "                current_group.append(s)\n",
        "                current_tokens += s_tokens\n",
        "        if current_group:\n",
        "            groups.append(\" \".join(current_group))\n",
        "        \n",
        "        summary_text = reduce_summaries(groups, round_num=2)\n",
        "else:\n",
        "    # small enough to summarize directly\n",
        "    summary_text = summarize(full_body_text)\n",
        "    summaries = [summary_text]\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"FINAL SUMMARY:\")\n",
        "print(f\"{'='*80}\")\n",
        "print(summary_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "# export summary to .txt document for benchmarking analysis:\n",
        "\n",
        "file_path = f\"./summaries/{cid}_bart_summary.txt\"\n",
        "\n",
        "with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(summary_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmarking Analysis\n",
        "\n",
        "- For grammar, readability, and clarity: \n",
        "  - [LanguageTool API](https://languagetool.org/http-api/) - Grammar and style checking\n",
        "  - [Textstat](https://textstat.org/) - Readability scores (Flesch-Kincaid, SMOG, etc.)\n",
        "  - [Perplexity (Hugging Face)](https://huggingface.co/docs/transformers/en/perplexity) - Fluency proxy via GPT-2\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPvgGZfwLldGEkhAgBwkRCJ",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
