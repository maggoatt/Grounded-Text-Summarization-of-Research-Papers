{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maggoatt/Grounded-Text-Summarization-of-Research-Papers/blob/main/Evidence_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H08W8bDq2h6Y"
      },
      "source": [
        "## Evidence Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Installations and imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install rank-bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Eibz_VKl0mi9"
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "import json, re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(s: str):\n",
        "    return re.findall(r\"[a-z0-9]+\", s.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"./data/249953535.json\", \"r\") as file:\n",
        "    paper = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "section_info = []\n",
        "raw_texts = []\n",
        "\n",
        "for section in paper[\"sections\"]:\n",
        "    # Include the title to make retrieval better\n",
        "    raw_text = f'{section[\"section_title\"]} {section[\"text\"]}'\n",
        "    curr_section = {\n",
        "        \"corpusId\": paper[\"corpusid\"],\n",
        "        \"title\": paper[\"title\"],\n",
        "        \"section_title\": section[\"section_title\"],\n",
        "        \"text\": section[\"text\"]\n",
        "    }\n",
        "    raw_texts.append(raw_text)\n",
        "    section_info.append(curr_section)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29\n"
          ]
        }
      ],
      "source": [
        "print(len(raw_texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_texts = [tokenize(raw_text) for raw_text in raw_texts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "bm25 = BM25Okapi(tokenized_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_sentence = \"\"\"Existing evaluations are too object-focused, artificially creating \"novelty\" by repurposing images designed for object classification. \"\"\"\n",
        "summary_scores = bm25.get_scores(tokenize(summary_sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_k = 5\n",
        "top_idxs = summary_scores.argsort()[::-1][:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus Id 249953535\n",
            "Paper Title:  NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds\n",
            "Section Title:  The contributions of this study are:\n",
            "Section Score:  19.355517706229957\n",
            "Section snippet:  1. A scene-focused dataset purpose-built for visual novelty detection. Existing evaluations are too object-focused, artificially creating \"novelty\" by repurposing images designed for object classification. In Sec. 4, we benchmark visual detectors on  ...\n",
            "Corpus Id 249953535\n",
            "Paper Title:  NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds\n",
            "Section Title:  Introduction\n",
            "Section Score:  14.733107416521557\n",
            "Section snippet:  Recent progress in computer vision (Krizhevsky et al., 2017) and vision-informed reinforcement learning (Mnih et al., 2015;Silver et al., 2018) is exciting but focused on tasks like classification or video game playing where the agent's goals are nar ...\n",
            "Corpus Id 249953535\n",
            "Paper Title:  NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds\n",
            "Section Title:  Related Work on Datasets for Novelties and Anomalies\n",
            "Section Score:  11.981380640041861\n",
            "Section snippet:  Object-focused datasets. Datasets showcasing distinct object types, such as CIFAR-10, Caltech-256, and ImageNet (Deng et al., 2009), originally intended for supervised classification, are often repurposed to assess novelty detection under a k-classes ...\n",
            "Corpus Id 249953535\n",
            "Paper Title:  NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds\n",
            "Section Title:  D.2 Performance Comparion Across Datasets for Visual Novelty Detection\n",
            "Section Score:  9.462235268553485\n",
            "Section snippet:  See Table D.2 for visual novelty detection results on other datasets. Our benchmark is more challenging than many existing visual novelty detection evaluations, with only the experiments by Cheng & Vasconcelos (2021) on CUB-200-2010and Abati et al. ( ...\n",
            "Corpus Id 249953535\n",
            "Paper Title:  NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds\n",
            "Section Title:  Limitations.\n",
            "Section Score:  7.70069721414692\n",
            "Section snippet:  Our benchmark focuses on one specific open world. We wish to avoid overclaiming about generality (Raji et al., 2021). Many other open worlds are possible. The kinds of visual properties we call novel emphasize object types. We could instead have vari ...\n"
          ]
        }
      ],
      "source": [
        "for i in top_idxs:\n",
        "    section = section_info[int(i)]\n",
        "    print(\"Corpus Id\", section[\"corpusId\"])\n",
        "    print(\"Paper Title: \", section[\"title\"])\n",
        "    print(\"Section Title: \", section[\"section_title\"])\n",
        "    print(\"Section Score: \", float(summary_scores[i]))\n",
        "    print(\"Section snippet: \", section[\"text\"][:250], \"...\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP+bIp2ZHmF9KIQ0YrxGDTH",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (cs178)",
      "language": "python",
      "name": "cs178"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
